{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02446c8f-8618-48a3-bdfc-9eda40e9ea7f",
   "metadata": {},
   "source": [
    "# Joint Fitting example\n",
    "\n",
    "This notebook covers how to perform joint inference and optimization for multiple observations, using ndspec's fit objects. This notebook has combines two analyses from previous tutorials and explores how constraints on parameters change when performing joint fitting and inference. See the following for more detailed explanations of fitting: [power spectra](https://ndspec.readthedocs.io/en/latest/fit_psd.html), [time-averaged spectra](https://ndspec.readthedocs.io/en/latest/fit_spec.html), [1-D cross-spectra](https://ndspec.readthedocs.io/en/latest/fit_cross_1d.html) and [2-D cross-spectra](https://ndspec.readthedocs.io/en/latest/fit_cross_2d.html).\n",
    "\n",
    "First, we'll import important packages (as well as set some specific environment variables that helps with performance when using numpy in parallel frameworks such as in emcee which is further explained in [performance considerations](https://ndspec.readthedocs.io/en/latest/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce178a77-a10d-40ae-9378-258992a83c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" \n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\" \n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\" \n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\" \n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\" \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ndspec.Response import ResponseMatrix\n",
    "import ndspec.FitCrossSpectrum as fitcross\n",
    "import ndspec.FitTimeAvgSpectrum as fitspec\n",
    "import ndspec.models as models\n",
    "import ndspec.XspecInterface as XSModels\n",
    "\n",
    "from lmfit import Model as LM_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4348ff-c88f-48ee-835a-9c3b2b420663",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "We'll be using the same data and performing the same data preparation as the earlier tutorials: [Fitting a time averaged spectrum](https://ndspec.readthedocs.io/en/latest/fit_spec.html) and [Fitting a one-dimensional cross-spectrum](https://ndspec.readthedocs.io/en/latest/fit_cross_1d.html). We'll load all of our data, and rebin the energy matrices into more appropriate bins for the cross-spectrum and time-averaged energy spectrum. See the previous tutorials for the full explanation of why and how we rebin the response matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1373fa99-c429-4986-83f9-444a6287a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and rebin response - we need this to have the exact intervals for our energy bins\n",
    "path = os.getcwd()\n",
    "\n",
    "rmfpath = path+\"/data/1200120106_rmf.pha\"\n",
    "nicer_matrix = ResponseMatrix(rmfpath)\n",
    "arfpath = path+\"/data/1200120106_arf.pha\"\n",
    "nicer_matrix.load_arf(arfpath)\n",
    "\n",
    "#deal with cross-spectrum\n",
    "nicer_energy_rebin = nicer_matrix.rebin_energies(10)\n",
    "\n",
    "rebin_bounds = np.geomspace(0.5,10,41)\n",
    "rebin_bounds = np.append(np.min(nicer_matrix.emin),rebin_bounds)\n",
    "rebin_bounds = np.append(rebin_bounds,np.max(nicer_matrix.emax))\n",
    "\n",
    "rebin_bounds_lo = rebin_bounds[:-1]\n",
    "rebin_bounds_hi = rebin_bounds[1:]\n",
    "rebin_response = nicer_matrix.rebin_channels(rebin_bounds_lo,rebin_bounds_hi)\n",
    "\n",
    "channel_grid = 0.5*(rebin_response.emax+rebin_response.emin)\n",
    "channel_width = rebin_response.emax-rebin_response.emin\n",
    "#finally, find the array of energy channel edges that line up with the response and are roughly geometrically spaced\n",
    "fine_channel_grid_edges = np.append(channel_grid-0.5*channel_width,channel_grid[-1]+0.5*channel_width[-1])\n",
    "\n",
    "#deal with energy spectrum\n",
    "rebin_matrix = nicer_matrix.rebin_energies(4)\n",
    "spectrum_fit = fitspec.FitTimeAvgSpectrum()\n",
    "\n",
    "spectrum_fit.set_data(rebin_matrix,os.getcwd()+'/data/1200120106_rebin.pha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fd9ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert from lightcurves to lag-energy spectra with Stingray\n",
    "from ndspec.SimpleFit import load_lc\n",
    "from stingray.fourier import avg_cs_from_timeseries, avg_pds_from_timeseries\n",
    "from stingray.fourier import poisson_level\n",
    "from stingray.fourier import error_on_averaged_cross_spectrum\n",
    "from stingray.utils import show_progress\n",
    "\n",
    "def lag_from_lcs(lc_strings,lc_ref,freq_lo,freq_hi,seg_size,time_res):\n",
    "    time_ref,counts_ref,gtis_ref = load_lc(lc_ref)\n",
    "    results = avg_pds_from_timeseries(\n",
    "            time_ref,\n",
    "            gtis_ref,\n",
    "            seg_size,\n",
    "            time_res,\n",
    "            silent=True,\n",
    "            norm=\"none\",\n",
    "            fluxes=counts_ref,)\n",
    "    freq = results[\"freq\"]\n",
    "    ref_power = results[\"power\"]\n",
    "    m_ave = results.meta[\"m\"]\n",
    "    ref_power_noise = poisson_level(norm=\"none\", n_ph=np.sum(counts_ref) / m_ave)\n",
    "    freq_mask = (freq>freq_lo) & (freq<freq_hi)\n",
    "    n_freqs = freq_mask[freq_mask==True].size\n",
    "    mean_ref_power = np.mean(ref_power[freq_mask])\n",
    "    m_tot = n_freqs * m_ave\n",
    "    f_mean = (freq_lo + freq_hi)*0.5\n",
    "\n",
    "    lag_spec = []\n",
    "    lag_spec_err = []\n",
    "\n",
    "    for i in range(len(lc_strings)):\n",
    "        time_sub,counts_sub,gtis_sub = load_lc(lc_strings[i])\n",
    "\n",
    "        results_cross = avg_cs_from_timeseries(\n",
    "                time_sub,\n",
    "                time_ref,\n",
    "                gtis_sub,\n",
    "                seg_size,\n",
    "                time_res,\n",
    "                silent=True,\n",
    "                norm=\"none\",\n",
    "                fluxes1=counts_sub,\n",
    "                fluxes2=counts_ref,\n",
    "            )\n",
    "\n",
    "        results_ps = avg_pds_from_timeseries(\n",
    "                time_sub,\n",
    "                gtis_sub,\n",
    "                seg_size,\n",
    "                time_res,\n",
    "                silent=True,\n",
    "                norm=\"none\",\n",
    "                fluxes=counts_sub,\n",
    "            )\n",
    "\n",
    "        sub_power_noise = poisson_level(\n",
    "            norm=\"none\", n_ph=np.sum(counts_sub) / results_ps.meta[\"m\"]\n",
    "        )\n",
    "        cross = results_cross[\"power\"]\n",
    "        sub_power = results_ps[\"power\"]\n",
    "        Cmean = np.mean(cross[freq_mask])\n",
    "        mean_sub_power = np.mean(sub_power[freq_mask])\n",
    "\n",
    "        _, _, phi_e, _ = error_on_averaged_cross_spectrum(\n",
    "                Cmean,\n",
    "                mean_sub_power,\n",
    "                mean_ref_power,\n",
    "                m_tot,\n",
    "                sub_power_noise,\n",
    "                ref_power_noise,\n",
    "                common_ref=True,\n",
    "            )\n",
    "\n",
    "        phase = np.angle(Cmean)\n",
    "        lag_spec = np.append(lag_spec,phase/(2*np.pi*f_mean))\n",
    "        lag_spec_err = np.append(lag_spec_err,phi_e/(2*np.pi*f_mean))\n",
    "\n",
    "\n",
    "\n",
    "    return lag_spec, lag_spec_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2961c9a8-f5bb-42c6-b745-cf59b254c408",
   "metadata": {},
   "outputs": [],
   "source": [
    "#round the bins of the channels in each lightcurve\n",
    "fine_channel_string = []\n",
    "for i in range(len(fine_channel_grid_edges)):\n",
    "    bin = \"%.0f\" % round(100.*fine_channel_grid_edges[i],2)\n",
    "    fine_channel_string = np.append(fine_channel_string,bin)\n",
    "fine_channel_string = np.array(fine_channel_string,dtype=int)\n",
    "\n",
    "#create the path to each lightcurve\n",
    "fine_full_string = []\n",
    "for i in range(len(fine_channel_string)-1):\n",
    "    bin_string = \"_\"+str(fine_channel_string[i])+\"-\"+str(fine_channel_string[i+1]-1)\n",
    "    lc_string = path+\"/data/lightcurves/ni1200120106mpu7_sr\" + bin_string + \".lc\"\n",
    "    fine_full_string = np.append(fine_full_string,lc_string)\n",
    "\n",
    "ref_path = path+\"/data/lightcurves/ni1200120106mpu7_sr_reference.lc\"\n",
    "\n",
    "#define the frequency bounds and, time resolution, reference band  and segment size, and load the data\n",
    "freqs = np.geomspace(0.2,16,7)\n",
    "\n",
    "lags = []\n",
    "lags_err = []\n",
    "\n",
    "dt = 0.03\n",
    "segment_size = 5\n",
    "ref_band = [0.5, 10]\n",
    "\n",
    "for i in show_progress(range(6)):\n",
    "    lag,lag_err=lag_from_lcs(fine_full_string,ref_path,freqs[i],freqs[i+1],segment_size,dt)\n",
    "    lags = np.append(lags,lag)\n",
    "    lags_err = np.append(lags_err,lag_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ca7cf1-c0cf-4fd2-bf73-7e8076ce41da",
   "metadata": {},
   "outputs": [],
   "source": [
    "lags_fit = fitcross.FitCrossSpectrum()\n",
    "lags_fit.set_coordinates(\"lags\")\n",
    "lags_fit.set_product_dependence(\"energy\")\n",
    "\n",
    "#nicer_matrix\n",
    "#nicer_energy_rebin\n",
    "lags_fit.set_data(nicer_energy_rebin,ref_band,fine_channel_grid_edges,\n",
    "                  lags,lags_err,\n",
    "                  freq_bins=freqs,\n",
    "                  time_res=dt,seg_size=segment_size)\n",
    "lags_fit.ignore_energies(0,0.5)\n",
    "lags_fit.ignore_energies(10.0,fine_channel_grid_edges[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74105cb8-0385-400b-ab5d-f95066023c72",
   "metadata": {},
   "source": [
    "### Define models\n",
    "So far, we've loaded in the data and created two Fit objects, each with their own data. We now want to model the data in the same way as [Fitting a time averaged spectrum](https://ndspec.readthedocs.io/en/latest/fit_spec.html) and [Fitting a one-dimensional cross-spectrum](https://ndspec.readthedocs.io/en/latest/fit_cross_1d.html), but instead jointly.\n",
    "\n",
    "First, we have to define our time-averaged spectrum and time-lags models using a mix of ndspec's phemonological models as well as xspec models that have been loaded by ndspec's xspec interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d12465-bba9-4798-bda5-255e75f528d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_library = XSModels.FortranInterface()\n",
    "\n",
    "def tbabs(ear, params):\n",
    "    pass\n",
    "\n",
    "def diskbb(ear, params):\n",
    "    pass\n",
    "\n",
    "model_library.load_models({\n",
    "    \"diskbb\": diskbb,\n",
    "    \"tbabs\": tbabs\n",
    "})\n",
    "\n",
    "# Define the models for the time-averaged spectrum\n",
    "def powerlaw(energ,index,norm_pl):\n",
    "    par_array = np.array([norm_pl,index])\n",
    "    model = models.powerlaw(energ,par_array)\n",
    "    if np.any(np.isnan(model)):\n",
    "        print(\"Bad PL\")\n",
    "    return model\n",
    "\n",
    "def wrap_diskbb(energ,kT,norm_disk):\n",
    "    par_array = np.array([kT,norm_disk])\n",
    "    model = model_library.diskbb(energ,par_array)\n",
    "    if np.any(np.isnan(model)):\n",
    "        print(\"Bad disk\")\n",
    "    return model\n",
    "\n",
    "#the xspec wrappers completely fuck up with emcee\n",
    "def blackbody(energ,kT,norm_disk):\n",
    "    par_array = np.array([norm_disk,kT])\n",
    "    model = models.bbody(energ,par_array)\n",
    "    \n",
    "    if np.any(np.isnan(model)):\n",
    "        print(\"Bad BB\")\n",
    "    return model\n",
    "\n",
    "def wrap_tbabs(energ,nH):\n",
    "    par_array = np.array([nH])\n",
    "    model = model_library.tbabs(energ,par_array)\n",
    "    if np.any(np.isnan(model)):\n",
    "        print(\"Bad tbabs\")\n",
    "    return model\n",
    "\n",
    "#Define the models for the time-lag cross-spectrum\n",
    "from ndspec.Timing import PowerSpectrum, CrossSpectrum\n",
    "\n",
    "def wrap_tbabs_tl(energs,nH):\n",
    "    par_array = np.array([nH])\n",
    "    model = model_library.tbabs(energs,par_array)\n",
    "    if np.any(np.isnan(model)):\n",
    "        print(\"Bad tbabs\")\n",
    "    return model\n",
    "\n",
    "#this is defined in the Fourier domain as a transfer function\n",
    "def pivoting_lowecut(energs,freqs,norm_pl2,index,gamma_0,gamma_slope,phi_0,phi_slope,nu_0,cutoff):\n",
    "    param_array = np.array([norm_pl2,index,gamma_0,gamma_slope,phi_0,phi_slope,nu_0])\n",
    "    model = np.transpose(models.pivoting_pl(freqs,energs,param_array))*np.exp(-cutoff/energs)\n",
    "    return model\n",
    "\n",
    "#the reverberation models are defined in the time domain, so we need to define a cross spectrum to\n",
    "#convert it to a transfer function to then return\n",
    "def reverb(energs,times,rev_norm,rev_temp,rise_slope,decay_slope,peak_time,temp_slope):\n",
    "    param_array = np.array([rev_norm,rev_temp,rise_slope,decay_slope,peak_time,temp_slope])\n",
    "    impulse_response = models.bbody_bkn(times,energs,param_array)\n",
    "    cross_spec = CrossSpectrum(times,energ=energs)\n",
    "    cross_spec.set_impulse(impulse_response)\n",
    "    cross_spec.transfer_from_irf()\n",
    "    model = np.transpose(cross_spec.trans_func)\n",
    "    return model\n",
    "\n",
    "psd = PowerSpectrum(lags_fit._times)\n",
    "#set Lorentzian parameters from the PSD fit\n",
    "l1_pars = np.array([0.056,0.31,0.251])\n",
    "l2_pars = np.array([0.85,0.09,0.164])\n",
    "l3_pars = np.array([2.52,0.35,0.117])\n",
    "psd_model = models.lorentz(psd.freqs,l1_pars)+models.lorentz(psd.freqs,l2_pars)+models.lorentz(psd.freqs,l3_pars)\n",
    "psd.power_spec = psd_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec06c45e-fa89-4009-b1d2-5a31604ce7ed",
   "metadata": {},
   "source": [
    "Then, we define the actual composite models now and set our _individual_ FitObjects to use those models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c0a1d5-e3e3-4551-ab9d-41387b408411",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the models for the time-averaged spectrum\n",
    "# the unfolded model is a composite of tbabs, powerlaw, and diskbb.\n",
    "# This would be \"tbabs * (powerlaw + diskbb)\" in XSPEC notation\n",
    "unfolded_model = LM_Model(wrap_tbabs)*(LM_Model(powerlaw)+LM_Model(wrap_diskbb))\n",
    "\n",
    "start_params = unfolded_model.make_params(index=dict(value=-1.65,min=-3.0,max=-1.0),\n",
    "                                          norm_pl=dict(value=6.5,min=1e-1,max=50.),\n",
    "                                          kT=dict(value=0.25,min=0.10,max=1),\n",
    "                                          norm_disk=dict(value=2e5,min=1e1,max=1e8),\n",
    "                                          nH=dict(value=0.1,min=1e-3,max=10))\n",
    "\n",
    "spectrum_fit.set_model(unfolded_model,params=start_params)\n",
    "\n",
    "# Set the models for the time-lags\n",
    "#the timing model is a composite of the tbabs, pivoting powerlaw, and reverberation\n",
    "timing_model = LM_Model(wrap_tbabs_tl)*(LM_Model(pivoting_lowecut,independent_vars=['energs','freqs'])+\n",
    "                                    LM_Model(reverb,independent_vars=['energs','times']))\n",
    "\n",
    "start_params = timing_model.make_params(norm_pl2=dict(value=1,min=1e-3,max=1e4,vary=False),\n",
    "                                        index=dict(value=-1.598,min=-1.8,max=-1.4,vary=False),\n",
    "                                        gamma_0=dict(value=0.046,min=0,max=0.5,vary=True),\n",
    "                                        gamma_slope=dict(value=-0.016,min=-0.1,max=0,vary=True),\n",
    "                                        phi_0=dict(value=-2.31,min=-np.pi,max=np.pi,vary=True),\n",
    "                                        phi_slope=dict(value=2.01,min=-3.,max=3.0,vary=True),\n",
    "                                        nu_0=dict(value=lags_fit.freq_bounds[0],min=0.0001,max=10,vary=False),\n",
    "                                        cutoff=dict(value=0.20,min=0.01,max=0.8,vary=False,expr='rev_temp'),\n",
    "                                        rev_norm=dict(value=0.85,min=0,max=1e4,vary=True),\n",
    "                                        rev_temp=dict(value=0.20,min=0.01,max=0.8,vary=False),\n",
    "                                        rise_slope=dict(value=4,min=0,max=10,vary=False),\n",
    "                                        decay_slope=dict(value=-1.46,min=-10,max=0,vary=True),\n",
    "                                        peak_time=dict(value=0.01,min=0.0,max=0.2,vary=False),\n",
    "                                        temp_slope=dict(value=-0.04,min=-3,max=0,vary=True),\n",
    "                                        nH=dict(value=0.093,min=1e-3,max=10)\n",
    "                                        )\n",
    "\n",
    "\n",
    "lags_fit.set_model(timing_model,model_type=\"transfer\")\n",
    "lags_fit.set_psd_weights(psd)\n",
    "lags_fit.set_params(start_params)\n",
    "no_renorm_params = lags_fit.model_params\n",
    "lags_fit.renorm_phases(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbed694d-970b-44b6-9c40-f97149229e04",
   "metadata": {},
   "source": [
    "# Combining the two\n",
    "\n",
    "Okay, now that we have two models, we want to try and share parameters between them. To do that, we're going to use ndspec's JointFit class. JointFit acts similar to a dictionary, in which it contains each FitObject next to a name, and adds functionality for performing joint fits and inference which we'll discuss later on. Importantly, you can still use the individual objects to perform individual fits like normal and you set the objects data and model individually (as we did above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccbba1a-9585-4fda-b428-7c8d9c1d6d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ndspec.JointFit import JointFit\n",
    "\n",
    "bigFit = JointFit()\n",
    "bigFit.add_fitobj(spectrum_fit,\"time_avg_spectrum\")\n",
    "bigFit.add_fitobj(lags_fit,\"time_lags\")\n",
    "bigFit.share_params(spectrum_fit,lags_fit,[\"index\",\"nH\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43903174",
   "metadata": {},
   "source": [
    "And then we can use this new JointFit object to fit the data in exactly the same way as we've done previously in the single observation case with a single line call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d134064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigFit.fit_data()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c71d769",
   "metadata": {},
   "source": [
    "Nice! We have fitted the data, sharing two parameters between the time-averaged spectrum model and the time-lags model. Let's plot our results. We can do this by referencing the name of the observation and using the standard plotting functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1090f680",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigFit[\"time_lags\"].plot_model_1d()\n",
    "plt.show()\n",
    "bigFit[\"time_lags\"].plot_model_2d(use_phase=True,return_plot=True)\n",
    "plt.show()\n",
    "bigFit[\"time_avg_spectrum\"].plot_model(units='eeunfold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7240c1",
   "metadata": {},
   "source": [
    "### MCMC\n",
    "\n",
    "As a small note, there is functionally no difference between using MCMC for a single power spectrum or time-averaged spectrum fit and joint fitting. There is no limit to the number of observations you can add to a single JointFit object, other than that imposed by your hardware. By default, joint fitting will evaluate all observations contained within the object, so if you want to only fit some of the observations together, you can pass a list of names of the FitObjects that you want to evaluate.\n",
    "\n",
    "Make sure that parameters intended to be shared between models share a name, otherwise add a prefix or suffic to the parameter names so that the software can differentiate between them. nDspec does not perform any tricks to increase performance for joint fits, so having many free parameters will mean that MCMC will take the typically long time to converge for high dimensional modelling space. Consider making your model able to produce multiple data products (time-averaged spectrum and cross-spectrum for instance) to reduce the number of necessary parameters to model your data if you are having issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
